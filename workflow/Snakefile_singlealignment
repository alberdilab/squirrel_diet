### Setup sample wildcard:
import os
import re
import pandas as pd
from glob import glob

SAMPLE = [os.path.basename(fn).replace("_1.fq.gz", "")
            for fn in glob(f"resources/reads/*_1.fq.gz")]

################################################################################
### Setup the desired outputs
rule all:
    input:
        "results/counts.tsv"

################################################################################
### Filter reads with fastp
rule fastp:
    input:
        r1 = "resources/reads/{sample}_1.fq.gz",
        r2 = "resources/reads/{sample}_2.fq.gz"
    output:
        r1 = temp("results/fastp/{sample}_1.fq.gz"),
        r2 = temp("results/fastp/{sample}_2.fq.gz"),
        fastp_html = "results/fastp/{sample}.html",
        fastp_json = "results/fastp/{sample}.json"
    threads:
        4
    resources:
        mem_mb=lambda wildcards, input, attempt: max(8*1024, int(input.size_mb * 5) * 2 ** (attempt - 1)),
        runtime=lambda wildcards, input, attempt: max(10, int(input.size_mb / 1024 * 10) * 2 ** (attempt - 1))
    message:
        "Using FASTP to trim adapters and low quality sequences for {wildcards.sample}"
    shell:
        """
        module load fastp/0.23.4
        fastp \
            --in1 {input.r1} --in2 {input.r2} \
            --out1 {output.r1} --out2 {output.r2} \
            --trim_poly_g \
            --trim_poly_x \
            --n_base_limit 5 \
            --qualified_quality_phred 20 \
            --length_required 60 \
            --thread {threads} \
            --html {output.fastp_html} \
            --json {output.fastp_json} \
            --adapter_sequence CTGTCTCTTATACACATCT \
            --adapter_sequence_r2 CTGTCTCTTATACACATCT
        """

################################################################################
## Index Unite database:
rule index_unite:
    input:
        "resources/database/sh_general_release_dynamic_s_all_19.02.2025.fasta"
    output:
        touch('resources/database/index.done') # Flag file
    params:
        database = "resources/database/sh_general_release_dynamic_s_all_19.02.2025"
    threads:
        24
    resources:
        mem_mb=lambda wildcards, input, attempt: max(8*1024, int(input.size_mb * 5) * 2 ** (attempt - 1)),
        runtime=lambda wildcards, input, attempt: max(10, int(input.size_mb / 1024 * 5) * 2 ** (attempt - 1))
    message:
        "Indexing unite database with Bowtie2"
    shell:
        """
        module load bowtie2/2.4.2
        # Index MAG gene catalogue
        bowtie2-build \
            --large-index \
            --threads {threads} \
            {input} {params.database}
        """

################################################################################
### Map non-host reads to DRAM genes files using Bowtie2
rule bowtie2_unite_mapping:
    input:
        idx = "resources/database/index.done",
        r1 = "results/fastp/{sample}_1.fq.gz",
        r2 = "results/fastp/{sample}_2.fq.gz",
    output:
        bam = "results/bowtie/{sample}.bam"
    params:
        database = "resources/database/sh_general_release_dynamic_s_all_19.02.2025"
    threads:
        20
    resources:
        mem_mb=lambda wildcards, input, attempt: max(8*1024, int(input.size_mb * 2) * 2 ** (attempt - 1)),
        runtime=lambda wildcards, input, attempt: max(10, int(input.size_mb / 1024 * 5) * 2 ** (attempt - 1))
    message:
        "Mapping {wildcards.sample} to unite using Bowtie2"
    shell:
        """
        # Map reads to MAGs using Bowtie2
        module load bowtie2/2.4.2 samtools/1.21
        bowtie2 \
            --time \
            --threads {threads} \
            -x {params.database} \
            -1 {input.r1} \
            -2 {input.r2} \
            --seed 1337 \
        | samtools sort -@ {threads} -o {output.bam}
        """

################################################################################
### Extract only reads with two or less mismatches + MAPQ>30 (no mismatches)
rule bowtie2_filtering:
    input:
        "results/bowtie/{sample}.bam"
    output:
        "results/filter/{sample}.bam"
    threads:
        1
    resources:
        mem_mb=lambda wildcards, input, attempt: max(8*1024, int(input.size_mb * 2) * 2 ** (attempt - 1)),
        runtime=lambda wildcards, input, attempt: max(10, int(input.size_mb / 1024 * 5) * 2 ** (attempt - 1))
    message:
        "Filtering bam file of {wildcards.sample}"
    shell:
        """
        module load samtools/1.21
        samtools view -h {input} | awk 'substr($0,1,1)=="@" || ($5 > 30 && $0 ~ /NM:i:/ && match($0, /NM:i:([0-9]+)/, arr) && arr[1] <= 2)' | samtools view -b -o {output}
        """

################################################################################
### Calculate the number of reads that mapped to MAG catalogue genes with CoverM
rule coverM_counts:
    input:
        expand("results/filter/{sample}.bam", sample=SAMPLE)
    output:
        gene_counts = "results/coverm/counts.tsv"
    threads:
        8
    resources:
        mem_mb=lambda wildcards, input, attempt: max(8*1024, int(input.size_mb * 10) * 2 ** (attempt - 1)),
        runtime=lambda wildcards, input, attempt: max(10, int(input.size_mb / 1024 * 2) * 2 ** (attempt - 1))
    message:
        "Calculating MAG gene mapping rate using CoverM"
    shell:
        """
        module load coverm/0.7.0
        coverm contig \
            -b {input} \
            -m count \
            -t {threads} \
            --proper-pairs-only \
            > {output.gene_counts}
        """

rule coverM_counts_filter:
    input:
        "results/coverm/counts.tsv"
    output:
        "results/counts.tsv"
    threads:
        1
    resources:
        mem_mb=32*1024,
        runtime=60
    run:
        # Read the TSV file
        df = pd.read_csv(input[0], sep='\t')

        # Rename 'Contig' to 'Taxon' (if it exists)
        df.rename(columns={'Contig': 'Taxon'}, inplace=True)

        # Function to extract the first part of the sample name
        def clean_column_name(col):
            match = re.match(r'([A-Za-z0-9]+)_', col)  # Extract prefix before first underscore
            return match.group(1) if match else col  # Keep original if no match

        # Apply cleaning to column names
        new_columns = ['Taxon'] + [clean_column_name(col) for col in df.columns[1:]]

        # Assign cleaned column names
        df.columns = new_columns

        # Remove duplicate column names (in case multiple columns map to the same prefix)
        df = df.loc[:, ~df.columns.duplicated()]

        # Filter rows where all numeric values (excluding "Taxon") are zero
        filtered_df = df.loc[~(df.iloc[:, 1:].eq(0).all(axis=1))]

        # Write the filtered data to a new TSV file
        filtered_df.to_csv(output[0], sep='\t', index=False)
